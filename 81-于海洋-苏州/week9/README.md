# Week9 神经网络推理

## 损失函数

- 均值平方差
- 交叉熵

## 泛化

- 拟合 测试误差与训练误差差距较小；
- 欠拟合 模型没有能够很好的表现数据的结构，而出现的拟合度不高的情况。模型不能在训练集上获得足够低的误差；
- 过拟合 模型过分的拟合训练样本，但对测试样本预测准确率不高的情况，也就是说模型泛化能力很差。训练误差和测试误差之间的差距太大；
- 不收敛 模型不是根据训练集训练得到的。

## 过拟合

### 概念

模型过分的拟合训练样本，但对测试样本预测准确率不高的情况，也就是说模型泛化能力很差。训练误差和测试误差之间的差距太大

- 过拟合会导致
    1. 造成模型比较复杂；
    2. 模型的泛化性能太差了，遇到了新的数据，用所得到的过拟合的模型，正确率是很差的。

### 原因

- 建模样本选取了错误的选样方法、样本标签等，或样本数量太少，所选取的样本数据不足以代表预定的分类规则
- 样本噪音干扰过大，使得机器将部分噪音认为是特征从而扰乱了预设的分类规则
- 假设的模型无法合理存在，或者说是无法达到假设成立的条件
- 参数太多导致模型复杂度过高
- 神经网络模型：
    1. 对样本数据可能存在分类决策面不唯一，随着学习的进行,BP算法使权值可能收敛过于复杂的决策面；
    2. 权值学习迭代次数足够多，拟合了训练数据中的噪声和训练样例中没有代表性的特征。

### 解决方案

- 减少特征, 删除与目标不相关特征，如一些特征选择方法
- Early stopping
- 更多的训练样本
- 重新清洗数据
- Dropout

## Early Stopping

在每一个Epoch结束时，计算validation data的accuracy，当accuracy不再提高时，就停止训练。

- 那么该做法的一个重点便是怎样才认为validation accuracy不再提高了呢？并不是说validation
  accuracy一降下来便认为不再提高了，因为可能经过这个Epoch后，accuracy降低了，但是随后的
  Epoch又让accuracy又上去了，所以不能根据一两次的连续降低就判断不再提高。
- 一般的做法是，在训练的过程中，记录到目前为止最好的validation accuracy，当连续10次Epoch
  （或者更多次）没达到最佳accuracy时，则可以认为accuracy不再提高了。此时便可以停止迭代了
  （Early Stopping）。
- 这种策略也称为“No-improvement-in-n”，n即Epoch的次数，可以根据实际情况取，如10、20、30

## Dropout

在神经网络中，dropout方法是通过修改神经网络本身结构来实现的：

1. 在训练开始时，随机删除一些（可以设定为1/2，也可以为1/3，1/4等）隐藏层神经元，即
   认为这些神经元不存在，同时保持输入层与输出层神经元的个数不变。
2. 然后按照BP学习算法对ANN中的参数进行学习更新（虚线连接的单元不更新，因为认为这
   些神经元被临时删除了）。这样一次迭代更新便完成了。下一次迭代中，同样随机删除一
   些神经元，与上次不一样，做随机选择。这样一直进行，直至训练结束。
   Dropout方法是通过修改ANN中隐藏层的神经元个数来防止ANN的过拟合。

-为什么Dropout能够减少过拟合：

1. Dropout是随机选择忽略隐层节点，在每个批次的训练过程，由于每次随机忽略的隐层节点都不同，这样就使每次训练的网络都是不一样的，
   每次训练都可以当做一个“新”模型；
2. 隐含节点都是以一定概率随机出现，因此不能保证每2个隐含节点每次都同时出现。这样权值的更新不再依赖有固定关系隐含节点的共同作用，阻止了某些特征仅仅在其他特定特征下才有
   效果的情况。

总结：Dropout是一个非常有效的神经网络模型平均方法，通过训练大量的不同的网络，来平均预测概率。不同的模型在不同的训练集上训练（每个epoch的训练数据都是随机选择），最后在每个模型用相同的权重来“融合”。

## Softmax

Softmax 函数通常作为神经网络中最后一层的激活函数，用于多类别分类问题。它将神经网络的原始输出转化为对应类别的概率。通过对最后一层的输出应用
Softmax 函数，可以使得神经网络在训练过程中更好地优化并输出适合于多类别分类的概率分布。

Softmax 函数的特点是它能够将原始输出转化为一个概率分布，使得每个输出元素都表示对应类别的概率。这使得 Softmax
在多类别分类任务中特别有用，例如图像分类、文本分类等。



# -